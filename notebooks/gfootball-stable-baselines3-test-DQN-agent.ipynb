{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# <center> GFootball Stable-Baselines3 </center>\n",
    "\n",
    "---\n",
    "<center><img src=\"https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/docs/_static/img/logo.png\" width=\"308\" height=\"268\" alt=\"Stable-Baselines3\"></center>\n",
    "<center><small>Image from Stable-Baselines3 repository</small></center>\n",
    "\n",
    "---\n",
    "This notebook uses the [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3) library to train a [PPO](https://openai.com/blog/openai-baselines-ppo/) reinforcement learning agent on [GFootball Academy](https://github.com/google-research/football/tree/master/gfootball/scenarios) scenarios, applying the architecture from the paper \"[Google Research Football: A Novel Reinforcement Learning Environment](https://arxiv.org/abs/1907.11180)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# sys.path.append(\"../imitation_learning\")\n",
    "import os\n",
    "import base64\n",
    "import pickle\n",
    "import zlib\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from torch import nn, tensor\n",
    "from collections import deque\n",
    "from gym.spaces import Box, Discrete\n",
    "# from kaggle_environments import make\n",
    "# from kaggle_environments.envs.football.helpers import *\n",
    "from gfootball.env import create_environment, observation_preprocessing, wrappers\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv\n",
    "from stable_baselines3.common.policies import BasePolicy, register_policy\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from datetime import date\n",
    "# from visualizer import visualize\n",
    "from matplotlib import pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "import torch\n",
    "from models.MlpClassifierModel import MlpClassifierModel\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.manual_seed(torch.initial_seed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Football Gym\n",
    "> [Stable-Baselines3: Custom Environments](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)<br/>\n",
    "> [SEED RL Agent](https://www.kaggle.com/piotrstanczyk/gfootball-train-seed-rl-agent): stacked observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "class FootballGym(gym.Env):\n",
    "    spec = None\n",
    "    metadata = None\n",
    "#     metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, config=None, render=False, rewards='scoring'):\n",
    "        super(FootballGym, self).__init__()\n",
    "        env_name = \"academy_empty_goal_close\"\n",
    "#         rewards = \"scoring,checkpoints\"\n",
    "\n",
    "        rewards = rewards\n",
    "        if config is not None:\n",
    "            env_name = config.get(\"env_name\", env_name)\n",
    "            rewards = config.get(\"rewards\", rewards)\n",
    "        self.env = create_environment(\n",
    "            env_name=env_name,\n",
    "            stacked=False,\n",
    "            representation=\"simple115v2\",\n",
    "            rewards = rewards,\n",
    "            write_goal_dumps=False,\n",
    "            write_full_episode_dumps=False,\n",
    "            render=render,\n",
    "            write_video=False,\n",
    "            dump_frequency=1,\n",
    "            logdir=\".\",\n",
    "            extra_players=None,\n",
    "            number_of_left_players_agent_controls=1,\n",
    "            number_of_right_players_agent_controls=0)\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.reward_range = (-1, 1)\n",
    "        self.obs_stack = deque([], maxlen=4)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.obs_stack.clear()\n",
    "        obs = self.env.reset()\n",
    "#         obs = self.transform_obs(obs)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step([action])\n",
    "#         obs = self.transform_obs(obs)\n",
    "        return obs, float(reward), done, info\n",
    "    \n",
    "# check_env(env=FootballGym(), warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootballMLP(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=115):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        self.mlp = MlpClassifierModel(hparams, input_size=115, p_dropout=0.25, num_classes=19)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        return self.mlp(input_tensor)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "hparams['hidden_size'] = 1024\n",
    "hparams['lr'] = 2e-3\n",
    "hparams['lr_decay_rate'] = 0.25\n",
    "hparams['batch_size'] = 256\n",
    "hparams['activation'] = 'GELU'\n",
    "# hparams['activation'] = 'ReLU'\n",
    "# model = MLPModel(hparams).to('cuda')\n",
    "# model = MlpClassifierModel(hparams).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class DDQN(DQN):\n",
    "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
    "        # Switch to train mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(True)\n",
    "        # Update learning rate according to schedule\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "\n",
    "        losses = []\n",
    "        for _ in range(gradient_steps):\n",
    "            ### YOUR CODE HERE\n",
    "            # Sample replay buffer\n",
    "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n",
    "\n",
    "            # Do not backpropagate gradient to the target network\n",
    "            with th.no_grad():\n",
    "#                 print(f\"replay data actions: {replay_data.actions.shape} and data: {replay_data.actions}\")\n",
    "                # Compute the next Q-values using the target network\n",
    "                next_q_values = self.q_net_target(replay_data.next_observations)\n",
    "#                 print(f\"Next Q values shape calculated by target net: {next_q_values.shape}\")\n",
    "                # Decouple action selection from value estimation\n",
    "                # Compute q-values for the next observation using the online q net\n",
    "                next_q_values_online = self.q_net(replay_data.next_observations)\n",
    "#                 print(f\"Next Q values shape calculated by Online net: {next_q_values_online.shape}\")\n",
    "                # Select action with online network\n",
    "                next_actions_online = next_q_values_online.argmax(dim=1)\n",
    "#                 print(f\"Next Actions shape calculated by Online net: {next_actions_online.shape}\")\n",
    "                # Estimate the q-values for the selected actions using target q network\n",
    "                next_q_values = th.gather(next_q_values, dim=1, index=next_actions_online.unsqueeze(-1))\n",
    "#                 print(f\"Next Q values calculated by Target net from the selected actions: {next_q_values.shape}\")\n",
    "               \n",
    "                # 1-step TD target\n",
    "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
    "\n",
    "            # Get current Q-values estimates\n",
    "            current_q_values = self.q_net(replay_data.observations)\n",
    "\n",
    "            # Retrieve the q-values for the actions from the replay buffer\n",
    "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
    "\n",
    "            # Check the shape\n",
    "            assert current_q_values.shape == target_q_values.shape\n",
    "\n",
    "            # Compute loss (L2 or Huber loss)\n",
    "            loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
    "\n",
    "            ### END OF YOUR CODE\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Optimize the q-network\n",
    "            self.policy.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip gradient norm\n",
    "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "            self.policy.optimizer.step()\n",
    "\n",
    "        # Increase update counter\n",
    "        self._n_updates += gradient_steps\n",
    "\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/loss\", np.mean(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Vanilla DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # This initialization concentrates most of the layers into the feature extractor\n",
    "# # and then leaves only a single layer for the prediction part\n",
    "\n",
    "# policy_kwargs = dict(features_extractor_class=FootballMLP,\n",
    "#                      features_extractor_kwargs=dict(features_dim=1024),\n",
    "#                     net_arch = [],\n",
    "#                     )\n",
    "# model_name = \"dqn\"\n",
    "# model = DQN(policy=\"MlpPolicy\", \n",
    "#             env=train_env, \n",
    "#             policy_kwargs=policy_kwargs, \n",
    "#             verbose=1,\n",
    "#             exploration_initial_eps=0.00,\n",
    "#             exploration_final_eps=0.0,\n",
    "#             target_update_interval=15000,\n",
    "# #             learning_rate=hparams['lr'],\n",
    "# #             batch_size=hparams['batch_size'],\n",
    "#             seed=42,\n",
    "#             tensorboard_log='tb_logs_DQN',\n",
    "#             learning_starts=100000,\n",
    "#            )\n",
    "# model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Configuring the DQN net like SB3's network structure\n",
    "# # Only one layer for the feature extraction and the rest for the Q value computation\n",
    "# from stable_baselines3 import DQN\n",
    "\n",
    "# policy_kwargs = dict(\n",
    "#     net_arch = [1024, 1024, 1024],\n",
    "#     activation_fn = torch.nn.GELU\n",
    "# )\n",
    "# model_name = \"dqn\"\n",
    "# model = DQN(policy=\"MlpPolicy\", \n",
    "#             env=train_env, \n",
    "#             policy_kwargs=policy_kwargs, \n",
    "#             verbose=1,\n",
    "#             exploration_initial_eps=0.00,\n",
    "#             exploration_final_eps=0.0,\n",
    "#             target_update_interval=15000,\n",
    "# #             learning_rate=hparams['lr'],\n",
    "# #             batch_size=hparams['batch_size'],\n",
    "#             seed=42,\n",
    "#             tensorboard_log='tb_logs_DQN',\n",
    "#             learning_starts=100000,\n",
    "#            )\n",
    "# model.policy\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # DDQN\n",
    "\n",
    "# policy_kwargs = dict(\n",
    "#     net_arch = [1024, 1024, 1024],\n",
    "#     activation_fn = torch.nn.ReLU\n",
    "# )\n",
    "\n",
    "# # policy_kwargs = dict(features_extractor_class=FootballMLP,\n",
    "# #                      features_extractor_kwargs=dict(features_dim=1024),\n",
    "# #                     net_arch = [],\n",
    "# #                     )\n",
    "# model_name = \"ddqn\"\n",
    "# model = DDQN(policy=\"MlpPolicy\", \n",
    "#             env=train_env, \n",
    "#             policy_kwargs=policy_kwargs, \n",
    "#             verbose=1,\n",
    "#             exploration_initial_eps=0.05,\n",
    "#             exploration_final_eps=0.05,\n",
    "#             target_update_interval=150000,\n",
    "# #             learning_rate=0.0000001,\n",
    "# #             batch_size=hparams['batch_size'],\n",
    "#             seed=42,\n",
    "#             tensorboard_log='tb_logs_DDQN',\n",
    "#             train_freq=3002,\n",
    "# #             learning_starts=100000,\n",
    "#            )\n",
    "\n",
    "# # With cartpole initialization\n",
    "# # model = DDQN(policy=\"MlpPolicy\", \n",
    "# #             env=train_env, \n",
    "# #             learning_rate=2.3e-3,\n",
    "# #             batch_size=64,\n",
    "# #             buffer_size=100000,\n",
    "# #             learning_starts=1000,\n",
    "# #             gamma=0.99,\n",
    "# #             target_update_interval=10,\n",
    "# #             train_freq=256,\n",
    "# #             gradient_steps=128,\n",
    "# #             exploration_fraction=0.16,\n",
    "# #             exploration_final_eps=0.04,\n",
    "# #             policy_kwargs=policy_kwargs,\n",
    "# #              tensorboard_log='tb_logs_DDQN',\n",
    "# #              seed=42,\n",
    "# #              verbose=1,\n",
    "# #            )\n",
    "# model.policy\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tetsing the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DDQN.load(\"../models/ddqn/ddqn_gfootball_8_20-05-2022-23-37-05.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = {0: \"academy_empty_goal_close\",\n",
    "             1: \"academy_empty_goal\",\n",
    "             2: \"academy_run_to_score\",\n",
    "             3: \"academy_run_to_score_with_keeper\",\n",
    "             4: \"academy_pass_and_shoot_with_keeper\",\n",
    "             5: \"academy_run_pass_and_shoot_with_keeper\",\n",
    "             6: \"academy_3_vs_1_with_keeper\",\n",
    "             7: \"academy_corner\",\n",
    "             8: \"academy_counterattack_easy\",\n",
    "             9: \"academy_counterattack_hard\",\n",
    "             10: \"academy_single_goal_versus_lazy\",\n",
    "             11: \"11_vs_11_kaggle\",\n",
    "             12: \"11_vs_11_stochastic\",\n",
    "             13: \"11_vs_11_easy_stochastic\",\n",
    "             14: \"11_vs_11_hard_stochastic\"}\n",
    "\n",
    "scenario_name = scenarios[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_set = {\n",
    "    0: \"idle\",\n",
    "    1: \"left\",\n",
    "    2: \"top_left\",\n",
    "    3: \"top\",\n",
    "    4: \"top_right\",\n",
    "    5: \"right\",\n",
    "    6: \"bottom_right\",\n",
    "    7: \"bottom\",\n",
    "    8: \"bottom_left\",\n",
    "    9: \"long_pass\",\n",
    "    10: \"high_pass\",\n",
    "    11: \"short_pass\",\n",
    "    12: \"shot\",\n",
    "    13: \"sprint\",\n",
    "    14: \"release_direction\",\n",
    "    15: \"release_sprint\",\n",
    "    16: \"sliding\",\n",
    "    17: \"dribble\",\n",
    "    18: \"release_dribble\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_match_with_dqn_agent(test_env):\n",
    "    obs = test_env.reset()\n",
    "    env_steps = 0\n",
    "    match_reward = 0\n",
    "    done = False\n",
    "    my_agent = 0\n",
    "    ai_agent = 0\n",
    "#     model.eval()\n",
    "    while not done:\n",
    "        action, state = model.predict(obs, deterministic=True)\n",
    "#         print(action)\n",
    "#         action, state = model.predict(obs)\n",
    "        obs, reward, done, info = test_env.step(action)\n",
    "        if reward > 0:\n",
    "            my_agent += 1\n",
    "        elif reward < 0:\n",
    "            ai_agent += 1\n",
    "#             print(f\"Step: {str(env_steps).ljust(10, ' ')}\\t{str(action_set[action.item()]).ljust(10, ' ')}\\t{round(reward,2)}\\t{info}\")\n",
    "        env_steps += 1\n",
    "        if (env_steps+1) % 3001  == 0:\n",
    "#             print(f\"Match reward: {match_reward}\")\n",
    "            return my_agent, ai_agent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO.load(\"ppo_gfootball\")\n",
    "\n",
    "# scenario_names = [scenarios[11], scenarios[12], scenarios[13], scenarios[14]]\n",
    "scenario_names = [scenarios[13]]\n",
    "# scenario_names = [scenarios[12], scenarios[13]]\n",
    "total_matches = 10\n",
    "with torch.no_grad():\n",
    "    for scenario_name in scenario_names:\n",
    "        test_env = FootballGym({\"env_name\":scenario_name}, render=False)\n",
    "        win_count = 0\n",
    "        draw_count = 0\n",
    "        total_goals_scored = 0\n",
    "        total_goal_conceded = 0\n",
    "        print(f\"-------------------------------------------------\")\n",
    "        print(f\"Playing in the {scenario_name} scenario:\")\n",
    "        print(f\"-------------------------------------------------\")\n",
    "        \n",
    "        for i in range(total_matches):\n",
    "#             my_agent_goals, ai_agent_goals = play_match(test_env)\n",
    "            my_agent_goals, ai_agent_goals = play_match_with_dqn_agent(test_env)\n",
    "            total_goals_scored += my_agent_goals\n",
    "            total_goal_conceded += ai_agent_goals\n",
    "            if my_agent_goals > ai_agent_goals:\n",
    "                match_result = \"WIN!\"\n",
    "                win_count += 1\n",
    "            elif my_agent_goals < ai_agent_goals:\n",
    "                match_result = \"DEFEAT!\"\n",
    "            else:\n",
    "                match_result = \"DRAW\"\n",
    "                draw_count += 1\n",
    "            print(f\"Match {i+1}: {match_result.ljust(10, ' ')} | MY AGENT {my_agent_goals} - {ai_agent_goals} AI\")\n",
    "        print(f\"Results of {total_matches} Matches in {scenario_name} scenario:\")\n",
    "        print(f\"WON: {win_count}, LOST: {total_matches-win_count-draw_count}, DREW: {draw_count} | total goals scored: {total_goals_scored}, total goals conceded: {total_goal_conceded}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
