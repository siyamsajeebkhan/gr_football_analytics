{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Doing necessary imports and setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install stable-baselines3 hydra-core torchtoolbox\n",
    "# pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from os.path import join, dirname\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imitation_learning.dataset.frame_dataset import Float115Dataset\n",
    "from models.mlp import MLPModel\n",
    "from utils.solver import Solver\n",
    "from gfootball.env.wrappers import Simple115StateWrapper\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from imitation_learning.dataset.dict_dataset import Float115Dataset as DictDataset\n",
    "\n",
    "\n",
    "\n",
    "# import hydra\n",
    "# import wandb\n",
    "# from omegaconf import DictConfig\n",
    "# import torchtoolbox.transform as transforms\n",
    "# import torchvision\n",
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loading Dataset from dict files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/frames.csv', header=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = np.split(dataset.sample(frac=1, random_state=42), [\n",
    "                                    int(.6 * len(dataset)), int(.8 * len(dataset))])\n",
    "\n",
    "train_frames, val_frames, test_frames = np.array(train, dtype='str'), np.array(val, dtype='str'), np.array(test, dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total memory occupied {dataset.nbytes/1000000 + train.nbytes/1000000 + val.nbytes/1000000 + test.nbytes/1000000} MB\")\n",
    "dataset.nbytes/1000000, train.nbytes/1000000, val.nbytes/1000000, test.nbytes/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/ssk/Study/GRP/dataset/npy_files'\n",
    "train_dataset, val_dataset, test_dataset = Float115Dataset(train_frames, dataset_path), \\\n",
    "    Float115Dataset(val_frames, dataset_path), \\\n",
    "        Float115Dataset(test_frames, dataset_path)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n",
    "print(\"Number of test samples:\", len(test_dataset))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(train)\n",
    "del(val)\n",
    "del(test)\n",
    "del(dataset)\n",
    "del(train_frames)\n",
    "del(val_frames)\n",
    "del(test_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 1e-3\n",
    "# hidden_size = 100\n",
    "# std = 1.\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "model = MLPModel()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer=optimizer, mode='max', patience=1, verbose=True, factor=0.2)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver = Solver(model, train_loader, val_loader, loss_func=criterion, optimizer=optimizer, learning_rate=lr, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_out, _ = model(X_test)\n",
    "# l1_loss = L1()\n",
    "# mse_loss = MSE()\n",
    "# print(\"L1 loss on test set AFTER training: {:,.0f}\".format(l1_loss(rescale(y_out), rescale(y_test))[0].mean() ))\n",
    "# print(\"MSE loss on test set AFTER training: {:,.0f}\".format(mse_loss(rescale(y_out), rescale(y_test))[0].mean() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_out = solver.get_dataset_prediction(test_loader)\n",
    "# l1_loss = L1()\n",
    "# mse_loss = MSE()\n",
    "# print(\"L1 loss on test set BEFORE training: {:,.0f}\".format(l1_loss(rescale(y_out), rescale(y_test))[0].mean() ))\n",
    "# print(\"MSE loss on test set BEFORE training: {:,.0f}\".format(mse_loss(rescale(y_out), rescale(y_test))[0].mean() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = MLPModel()\n",
    "# model = model.to(device)\n",
    "\n",
    "# optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# scheduler = ReduceLROnPlateau(optimizer=optim, mode='max', patience=1, verbose=True, factor=0.2)\n",
    "# criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=\"GRF_imitation_learning\")\n",
    "# wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add test data to test before training\n",
    "# X_test = [test_dataset[i] for i in range((len(test_dataset)))]\n",
    "# X_test = np.stack(X_test, axis=0)\n",
    "# y_test = [test_dataset[i]['target'] for i in range((len(test_dataset)))]\n",
    "# y_test = np.stack(y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs):\n",
    "    train_loss_values = []\n",
    "    eval_loss_values = []\n",
    "    training_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        correct = 0\n",
    "        correct_eval_samples = 0\n",
    "        epoch_loss = 0\n",
    "        eval_loss = 0\n",
    "        running_train_loss = 0.0\n",
    "        running_eval_loss = 0.0\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        print(\"Running the training\")\n",
    "        start = time.perf_counter()\n",
    "        for i, (x, y) in enumerate(tqdm(train_loader)):\n",
    "            x = torch.as_tensor(x, device=device, dtype=torch.float32)\n",
    "            y = torch.as_tensor(y, device=device, dtype=torch.float32)\n",
    "            optimizer.zero_grad()\n",
    "            end = time.perf_counter()\n",
    "            z = model(x)\n",
    "            loss = criterion(z, y.long())\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pred = torch.argmax(z, dim=1)\n",
    "#             if (i+1) % 100 == 0:\n",
    "#                 print(f\"train loss: {loss.item()}\", end='\\n')\n",
    "            correct += (pred.cpu() == y.long().cpu()).sum()\n",
    "            running_train_loss += loss.item() * x.shape[0]\n",
    "\n",
    "        epoch_accuracy = correct/len(train_dataset)\n",
    "        epoch_loss = running_train_loss / len(train_dataset)\n",
    "        train_loss_values.append(epoch_loss)\n",
    "        training_accuracy.append(epoch_accuracy)\n",
    "\n",
    "        # Eval\n",
    "        model.eval()\n",
    "        print(\"Running the validation\")\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(tqdm(val_loader)):\n",
    "                x = torch.as_tensor(x, device=device, dtype=torch.float32)\n",
    "                y = torch.as_tensor(y, device=device, dtype=torch.float32)\n",
    "                z = model(x)\n",
    "                loss = criterion(z, y.long())\n",
    "                running_eval_loss += loss.item() * x.shape[0]\n",
    "                writer.add_scalar('Loss/test', eval_loss, epoch)\n",
    "        eval_loss = running_eval_loss / len(val_dataset)\n",
    "        eval_loss_values.append(eval_loss)\n",
    "        print('Epoch {:03}/{}: | Train Loss: {:.3f} | Eval Loss: {:.3f} | Epoch accuracy: {:.2f} | Training time: {}'.format(\n",
    "            epoch + 1, num_epochs, epoch_loss, eval_loss, epoch_accuracy, str(datetime.timedelta(seconds=time.time() - start_time))[:7]))\n",
    "        # wandb.log({'train loss': epoch_loss, 'val loss': eval_loss})\n",
    "    return train_loss_values, eval_loss_values, training_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss_values, eval_loss_values, training_accuracy = train_model(50)\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a sanity check\n",
    "len(train_loss_values), len(eval_loss_values), len(training_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../saved_models/mlp_model.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../saved_models/mlp_full_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss_values, label='train_loss')\n",
    "plt.plot(eval_loss_values, label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig('imitation.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_accuracy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay_files_path = '../data/replay_files'\n",
    "# replay_files = sorted(os.listdir(replay_files_path))\n",
    "# replay_files.pop(0)\n",
    "# # replay_files = replay_files[0:1]\n",
    "# print(f\"total replay files: {len(replay_files)}\")\n",
    "# # replay_files = replay_files[0:1]\n",
    "\n",
    "# start = time.perf_counter()\n",
    "# prepare_dict_of_obs_from_replay_files(replay_files, replay_files_path)\n",
    "# end =  time.perf_counter()\n",
    "\n",
    "# print(f\"Total time needed to process {len(replay_files)}: {end-start}s\")\n",
    "# print(f\"Time needed to process a single file: {(end-start)/len(replay_files)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### !!!!!!!!!!!!!!!!!!! Do not delete this cell !!!!!!!!!!!!!!!!!!!\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from gfootball.env.wrappers import Simple115StateWrapper\n",
    "\n",
    "# Here we will write the pickle files\n",
    "def prepare_dict_of_obs_from_replay_files(replay_files, replay_files_path):\n",
    "    obs_save_dir = '/home/ssk/Study/GRP/dataset/dict_files'\n",
    "    # replay_files_path = 'dataset/replay_files'\n",
    "\n",
    "    if not os.path.exists(obs_save_dir):\n",
    "        os.mkdir(obs_save_dir)\n",
    "\n",
    "    for replay in tqdm(replay_files):\n",
    "        replay_dict = {}\n",
    "        with open(os.path.join(replay_files_path, replay), 'rb') as pkl_file:\n",
    "            episode_data = pickle.load(pkl_file)\n",
    "\n",
    "        episode_no = replay.split('.')[0]\n",
    "        episode = episode_data['observations']\n",
    "        episode['active'] = episode_data['players'][0]['active']\n",
    "        episode_length = 3002\n",
    "        raw_obs = {}\n",
    "\n",
    "#         episode_dir = os.path.join(obs_save_dir, episode_no)\n",
    "\n",
    "        for step in range(episode_length):\n",
    "            for (key, item) in episode.items():\n",
    "                raw_obs[key] = item[step]\n",
    "\n",
    "            float115_frame =  Simple115StateWrapper.convert_observation([raw_obs], True)[0].tolist()\n",
    "            action = episode_data['players'][0]['action'][step]\n",
    "            \n",
    "            frame_name = episode_no+f'_{step}'\n",
    "            if len(action) != 0:\n",
    "                float115_frame.extend(action)\n",
    "                replay_dict[frame_name] = np.array(float115_frame)\n",
    "#                 fram_save_path = os.path.join(episode_dir, frame_name)\n",
    "#                 np.save(fram_save_path, np.array(float115_frame))\n",
    "        \n",
    "#         dict_save_path = os.path.join(replay_dict, replay)\n",
    "        with open(os.path.join(obs_save_dir, replay), 'wb') as f:\n",
    "            pickle.dump(replay_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionaries of obs for each episode file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ssk/Study/GRP/dataset/dict_files/3720620.p', 'rb') as handle:\n",
    "    obs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs['3720620_1'][0:115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_batched_data():\n",
    "#     root_save_dir = '/home/ssk/Study/GRP/dataset/batched_data'\n",
    "#     train_save_dir = join(root_save_dir, 'train')\n",
    "#     val_save_dir = join(root_save_dir, 'val')\n",
    "#     test_save_dir = join(root_save_dir, 'test')\n",
    "    \n",
    "#     if not os.path.exists(root_save_dir):\n",
    "#         os.mkdir(root_save_dir)\n",
    "#     if not os.path.exists(train_save_dir):\n",
    "#         os.mkdir(train_save_dir)\n",
    "#     if not os.path.exists(val_save_dir):\n",
    "#         os.mkdir(val_save_dir)\n",
    "#     if not os.path.exists(test_save_dir):\n",
    "#         os.mkdir(test_save_dir)\n",
    "    # Train\n",
    "    start = time.perf_counter()\n",
    "    print(f\"Processing Train dataset\")\n",
    "    train_batch_file_prefix = 'train_batch'\n",
    "    for i, (x, y) in enumerate(tqdm(train_loader)):\n",
    "        if i==50:\n",
    "            break\n",
    "        end = time.perf_counter()\n",
    "        del(x)\n",
    "        del(y)\n",
    "    print(f\"Time to load {i} batches containing {128 * i} datapoints: {(end-start):.4} s\")\n",
    "    print(f\"Average Time to load a single batch containing {128} datapoints: {(end-start)/(i):.4f} s\")\n",
    "#         print(f\"Size of the read batch: {x.element_size() * x.nelement()/1000000} MB\")\n",
    "#         train_batch_name = \n",
    "#         train = {}\n",
    "#         train['obs'] = x\n",
    "#         train['act'] = y\n",
    "        \n",
    "\n",
    "    # Val\n",
    "#     print(f\"Processing Val dataset\")\n",
    "#     for i, (x, y) in enumerate(val_loader):\n",
    "#         pass\n",
    "#     # Test\n",
    "#     print(f\"Processing Test dataset\")\n",
    "#     for i, (x, y) in enumerate(test_loader):"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
