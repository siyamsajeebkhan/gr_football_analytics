{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import base64\n",
    "import pickle\n",
    "import zlib\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from torch import nn, tensor\n",
    "from collections import deque\n",
    "from gym.spaces import Box, Discrete\n",
    "from gfootball.env import create_environment, observation_preprocessing, wrappers\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv\n",
    "from stable_baselines3.common.policies import BasePolicy, register_policy\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from datetime import date\n",
    "from matplotlib import pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.manual_seed(torch.initial_seed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Football Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "class FootballGym(gym.Env):\n",
    "    spec = None\n",
    "    metadata = None\n",
    "#     metadata = {'render.modes': ['human']}\n",
    "    def __init__(self, config=None, render=False, rewards='scoring'):\n",
    "        super(FootballGym, self).__init__()\n",
    "        env_name = \"academy_empty_goal_close\"\n",
    "        rewards = rewards\n",
    "        if config is not None:\n",
    "            env_name = config.get(\"env_name\", env_name)\n",
    "            rewards = config.get(\"rewards\", rewards)\n",
    "        self.env = create_environment(\n",
    "            env_name=env_name,\n",
    "            stacked=False,\n",
    "            representation=\"simple115v2\",\n",
    "            rewards = rewards,\n",
    "            write_goal_dumps=False,\n",
    "            write_full_episode_dumps=False,\n",
    "            render=render,\n",
    "            write_video=False,\n",
    "            dump_frequency=1,\n",
    "            logdir=\".\",\n",
    "            extra_players=None,\n",
    "            number_of_left_players_agent_controls=1,\n",
    "            number_of_right_players_agent_controls=0)\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.reward_range = (-1, 1)\n",
    "        self.obs_stack = deque([], maxlen=4)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.obs_stack.clear()\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step([action])\n",
    "        return obs, float(reward), done, info\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the DDQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class DDQN(DQN):\n",
    "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
    "        # Switch to train mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(True)\n",
    "        # Update learning rate according to schedule\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "\n",
    "        losses = []\n",
    "        for _ in range(gradient_steps):\n",
    "            ### YOUR CODE HERE\n",
    "            # Sample replay buffer\n",
    "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n",
    "\n",
    "            # Do not backpropagate gradient to the target network\n",
    "            with th.no_grad():\n",
    "                # Compute the next Q-values using the target network\n",
    "                next_q_values = self.q_net_target(replay_data.next_observations)\n",
    "        \n",
    "                # Decouple action selection from value estimation\n",
    "                # Compute q-values for the next observation using the online q net\n",
    "                next_q_values_online = self.q_net(replay_data.next_observations)\n",
    "\n",
    "                # Select action with online network\n",
    "                next_actions_online = next_q_values_online.argmax(dim=1)\n",
    "\n",
    "                # Estimate the q-values for the selected actions using target q network\n",
    "                next_q_values = th.gather(next_q_values, dim=1, index=next_actions_online.unsqueeze(-1))\n",
    "               \n",
    "                # 1-step TD target\n",
    "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
    "\n",
    "            # Get current Q-values estimates\n",
    "            current_q_values = self.q_net(replay_data.observations)\n",
    "\n",
    "            # Retrieve the q-values for the actions from the replay buffer\n",
    "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
    "\n",
    "            # Check the shape\n",
    "            assert current_q_values.shape == target_q_values.shape\n",
    "\n",
    "            # Compute Huber loss (less sensistive to outliers)\n",
    "            loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
    "            \n",
    "            # Using KL-loss with Huber Loss.\n",
    "            # Uncomment this line if you want to use KL loss along with Huber loss\n",
    "#             kl_loss = th.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "#             loss = loss + kl_loss(F.log_softmax(current_q_values), F.softmax(target_q_values))\n",
    "            \n",
    "            # KL-loss only\n",
    "#             loss =  kl_loss(F.log_softmax(current_q_values), F.softmax(target_q_values))\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Optimize the q-network\n",
    "            self.policy.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradient norm\n",
    "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "            self.policy.optimizer.step()\n",
    "\n",
    "        # Increase update counter\n",
    "        self._n_updates += gradient_steps\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/loss\", np.mean(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment creation and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "scenarios = {0: \"academy_empty_goal_close\",\n",
    "             1: \"academy_empty_goal\",\n",
    "             2: \"academy_run_to_score\",\n",
    "             3: \"academy_run_to_score_with_keeper\",\n",
    "             4: \"academy_pass_and_shoot_with_keeper\",\n",
    "             5: \"academy_run_pass_and_shoot_with_keeper\",\n",
    "             6: \"academy_3_vs_1_with_keeper\",\n",
    "             7: \"academy_corner\",\n",
    "             8: \"academy_counterattack_easy\",\n",
    "             9: \"academy_counterattack_hard\",\n",
    "             10: \"academy_single_goal_versus_lazy\",\n",
    "             11: \"11_vs_11_kaggle\",\n",
    "             12: \"11_vs_11_stochastic\",\n",
    "             13: \"11_vs_11_easy_stochastic\",\n",
    "             14: \"11_vs_11_hard_stochastic\"}\n",
    "\n",
    "scenario_name = scenarios[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "def make_env(config: dict, rank: int, log_save_dir: str, seed: int = 42) -> Callable:\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "    \n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    :return: (Callable)\n",
    "    \"\"\"\n",
    "    def _init() -> gym.Env:\n",
    "        env = FootballGym(config, rewards='scoring')\n",
    "#         env = FootballGym(config, rewards='scoring,checkpoints')\n",
    "        log_file = os.path.join(log_save_dir, str(rank))\n",
    "        env = Monitor(env, log_file, allow_early_resets=True)\n",
    "        env.seed(seed + rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the vectorized training environmewnt and also creating the direcotry for logging\n",
    "\n",
    "timestamp = time.strftime('%d-%m-%Y-%H-%M-%S', time.localtime())\n",
    "print(timestamp)\n",
    "\n",
    "n_envs = 8\n",
    "config={\"env_name\":scenario_name}\n",
    "log_save_dir = os.path.join(\"../logs/dqn_logs\", timestamp)\n",
    "print(f\"Log dir: {log_save_dir}\")\n",
    "os.mkdir(log_save_dir)\n",
    "train_env = SubprocVecEnv([make_env(config, rank=i, log_save_dir=log_save_dir) for i in range(n_envs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Vanilla DQN model\n",
    "--- \n",
    "Uncomment the following cell if you want to use the vanilla DQN instead of Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configuring the DQN net like SB3's network structure\n",
    "# Only one layer for the feature extraction and the rest for the Q value computation\n",
    "# from stable_baselines3 import DQN\n",
    "\n",
    "# policy_kwargs = dict(\n",
    "#     net_arch = [1024, 1024, 1024],\n",
    "#     activation_fn = torch.nn.GELU\n",
    "# )\n",
    "# model_name = \"dqn\"\n",
    "# model = DQN(policy=\"MlpPolicy\", \n",
    "#             env=train_env, \n",
    "#             policy_kwargs=policy_kwargs, \n",
    "#             verbose=1,\n",
    "#             exploration_initial_eps=0.00,\n",
    "#             exploration_final_eps=0.0,\n",
    "#             target_update_interval=15000,\n",
    "# #             learning_rate=hparams['lr'],\n",
    "# #             batch_size=hparams['batch_size'],\n",
    "#             seed=42,\n",
    "#             tensorboard_log='tb_logs_DQN',\n",
    "#             learning_starts=100000,\n",
    "#            )\n",
    "# model.policy\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DDQN\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch = [1024, 1024, 1024],\n",
    "    activation_fn = torch.nn.ReLU\n",
    ")\n",
    "\n",
    "# policy_kwargs = dict(features_extractor_class=FootballMLP,\n",
    "#                      features_extractor_kwargs=dict(features_dim=1024),\n",
    "#                     net_arch = [],\n",
    "#                     )\n",
    "model_name = \"ddqn\"\n",
    "model = DDQN(policy=\"MlpPolicy\", \n",
    "            env=train_env, \n",
    "            policy_kwargs=policy_kwargs, \n",
    "            verbose=1,\n",
    "            exploration_initial_eps=0.05,\n",
    "            exploration_final_eps=0.05,\n",
    "            target_update_interval=150000,\n",
    "#             learning_rate=0.0000001,\n",
    "#             batch_size=hparams['batch_size'],\n",
    "            seed=42,\n",
    "            tensorboard_log='../logs/tb_logs_DDQN',\n",
    "            train_freq=3002,\n",
    "#             learning_starts=100000,\n",
    "           )\n",
    "\n",
    "model.policy\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading IL agent's weights\n",
    "---\n",
    "Download the checkpoints from here: [IL agent checkpoints](https://drive.google.com/drive/folders/1QwyPsWdGfJMhjEcBIhNot15iij_VRx_U?usp=sharing)\n",
    "\n",
    "Modify the path of the checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IL agent with BN\n",
    "# checkpoint_path = \"../il_agent_checkpoints/epoch=208-step=681548.ckpt\"\n",
    "\n",
    "# IL agent without BN\n",
    "checkpoint_path = \"../il_agent_checkpoints/epoch=146-step=479366.ckpt\"\n",
    "checkpoint_dict = torch.load(checkpoint_path)\n",
    "checkpoint_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_keys_todqn_keys_dict = {}\n",
    "sd_dqn_model = model.policy.state_dict()\n",
    "count = 0\n",
    "for mlp_key, dqn_key in zip(checkpoint_dict['state_dict'].keys(), model.policy.q_net.state_dict().keys()):\n",
    "    sd_dqn_model['q_net.' + dqn_key] = checkpoint_dict['state_dict'][mlp_key]\n",
    "for mlp_key, dqn_key in zip(checkpoint_dict['state_dict'].keys(), model.policy.q_net_target.state_dict().keys()):\n",
    "    sd_dqn_model['q_net_target.' + dqn_key] = checkpoint_dict['state_dict'][mlp_key]\n",
    "    \n",
    "model.policy.load_state_dict(sd_dqn_model)\n",
    "\n",
    "# Check the model's weights after loading the weights from IL agent\n",
    "model.policy.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Freezing the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For freezing the layers\n",
    "\n",
    "# for idx, param in enumerate(model.policy.parameters()):\n",
    "# #     if param.shape == torch.Size([19, 1024]) or param.shape == torch.Size([19]):\n",
    "#     if param.shape == torch.Size([19]):\n",
    "#         print(param.shape, param.requires_grad)\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False\n",
    "#         #     print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, param in enumerate(model.policy.parameters()):\n",
    "#     print(param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "class ProgressBar(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(ProgressBar, self).__init__(verbose)\n",
    "        self.pbar = None\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        factor = np.ceil(self.locals['total_timesteps'] / n_steps)\n",
    "        print(f\"self.locals['total_timesteps']:{self.locals['total_timesteps']}, n_steps: {n_steps}\")\n",
    "        n = 1\n",
    "        try:\n",
    "            n = len(self.training_env.envs)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                n = len(self.training_env.remotes)\n",
    "            except AttributeError:\n",
    "                n = 1\n",
    "        total = int(n_steps * factor / n)\n",
    "        self.pbar = tqdm(total=total)\n",
    "\n",
    "    def _on_rollout_start(self):\n",
    "        self.pbar.refresh()\n",
    "\n",
    "    def _on_step(self):\n",
    "        self.pbar.update(1)\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        self.pbar.refresh()\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        self.pbar.close()\n",
    "        self.pbar = None\n",
    "\n",
    "progressbar = ProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_epochs = 1\n",
    "n_steps = 3002\n",
    "total_timesteps = n_steps * n_envs * total_epochs\n",
    "model.learn(total_timesteps=total_timesteps, callback=progressbar, log_interval=8, tb_log_name='ddqn-test')\n",
    "\n",
    "\n",
    "saved_model_name = model_name + '_gfootball_' + str(n_envs) + \"_\" + timestamp\n",
    "model.save(f\"../models/{model_name}/{saved_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ../logs/tb_logs_DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the rewards per timestep and per episode\n",
    "\n",
    "plt.style.use(['seaborn-whitegrid'])\n",
    "results_plotter.plot_results([log_save_dir], total_timesteps, results_plotter.X_TIMESTEPS, \"GFootball Timesteps\")\n",
    "# plt.savefig('../figures/dqn/rewards_per_timestamp_dqn_with_my_policy.png')\n",
    "results_plotter.plot_results([log_save_dir], total_timesteps, results_plotter.X_EPISODES, \"GFootball Episodes\")\n",
    "# plt.savefig('../figures/dqn/rewards_per_episode_dqn_with_my_policy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the episodic reward with line\n",
    "\n",
    "x, y = results_plotter.ts2xy(results_plotter.load_results(log_save_dir), 'timesteps')  # Organising the logged results in to a clean format for plotting.\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.plot(x,y)\n",
    "plt.ylim([-10, 10])\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Episode Rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the rolling mean reward per environment\n",
    "\n",
    "plt.style.use(['seaborn-whitegrid'])\n",
    "log_files = [os.path.join(log_save_dir, f\"{i}.monitor.csv\") for i in range(n_envs)]\n",
    "\n",
    "nrows = np.ceil(n_envs/2)\n",
    "fig = plt.figure(figsize=(8, 2 * nrows))\n",
    "for i, log_file in enumerate(log_files):\n",
    "    if os.path.isfile(log_file):\n",
    "        df = pd.read_csv(log_file, skiprows=1)\n",
    "        plt.subplot(nrows, 2, i+1, label=log_file)\n",
    "        df['r'].rolling(window=5).mean().plot(title=f\"Rewards: Env {i}\")\n",
    "        plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean episodic reward\n",
    "# Download the CSV from tensorboard and put the path here\n",
    "df = pd.read_csv('data_for_figures/run-ddqn_3-tag-rollout_ep_rew_mean.csv')\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "df.plot(x ='Step', y='Value')\n",
    "# plt.savefig('../figures/dqn/mean_ep_reward_with_IL_50_epochs_diff_net.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
