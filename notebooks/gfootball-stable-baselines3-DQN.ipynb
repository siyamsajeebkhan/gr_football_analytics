{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "atari:\n",
    "  env_wrapper:\n",
    "    - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
    "  frame_stack: 4\n",
    "  policy: 'CnnPolicy'\n",
    "  n_timesteps: !!float 1e7\n",
    "  buffer_size: 100000\n",
    "  learning_rate: !!float 1e-4\n",
    "  batch_size: 32\n",
    "  learning_starts: 100000\n",
    "  target_update_interval: 1000\n",
    "  train_freq: 4\n",
    "  gradient_steps: 1\n",
    "  exploration_fraction: 0.1\n",
    "  exploration_final_eps: 0.01\n",
    "  optimize_memory_usage: True\n",
    "\n",
    "# Almost Tuned\n",
    "CartPole-v1:\n",
    "  n_timesteps=5e4,\n",
    "  policy='MlpPolicy',\n",
    "  learning_rate=float 2.3e-3,\n",
    "  batch_size=64,\n",
    "  buffer_size=100000,\n",
    "  learning_starts=1000,\n",
    "  gamma=0.99,\n",
    "  target_update_interval=10,\n",
    "  train_freq=256,\n",
    "  gradient_steps=128,\n",
    "  exploration_fraction=0.16,\n",
    "  exploration_final_eps=0.04,\n",
    "  policy_kwargs=policy_kwargs,\n",
    "\n",
    "# Tuned\n",
    "MountainCar-v0:\n",
    "  n_timesteps=1.2e5,\n",
    "  policy='MlpPolicy',\n",
    "  learning_rate=float 4e-3,\n",
    "  batch_size=128,\n",
    "  buffer_size=10000,\n",
    "  learning_starts=1000,\n",
    "  gamma=0.98,\n",
    "  target_update_interval=600,\n",
    "  train_freq=16,\n",
    "  gradient_steps=8,\n",
    "  exploration_fraction=0.2,\n",
    "  exploration_final_eps=0.07,\n",
    "  policy_kwargs=policy_kwargs,\n",
    "\n",
    "# Tuned\n",
    "LunarLander-v2:\n",
    "  n_timesteps: !!float 1e5\n",
    "  policy: 'MlpPolicy'\n",
    "  learning_rate: !!float 6.3e-4\n",
    "  batch_size: 128\n",
    "  buffer_size: 50000\n",
    "  learning_starts: 0\n",
    "  gamma: 0.99\n",
    "  target_update_interval: 250\n",
    "  train_freq: 4\n",
    "  gradient_steps: -1\n",
    "  exploration_fraction: 0.12\n",
    "  exploration_final_eps: 0.1\n",
    "  policy_kwargs: \"dict(net_arch=[256, 256])\"\n",
    "\n",
    "# Tuned\n",
    "Acrobot-v1:\n",
    "  n_timesteps: !!float 1e5\n",
    "  policy: 'MlpPolicy'\n",
    "  learning_rate: !!float 6.3e-4\n",
    "  batch_size: 128\n",
    "  buffer_size: 50000\n",
    "  learning_starts: 0\n",
    "  gamma: 0.99\n",
    "  target_update_interval: 250\n",
    "  train_freq: 4\n",
    "  gradient_steps: -1\n",
    "  exploration_fraction: 0.12\n",
    "  exploration_final_eps: 0.1\n",
    "  policy_kwargs: \"dict(net_arch=[256, 256])\"\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# <center> GFootball Stable-Baselines3 </center>\n",
    "\n",
    "---\n",
    "<center><img src=\"https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/docs/_static/img/logo.png\" width=\"308\" height=\"268\" alt=\"Stable-Baselines3\"></center>\n",
    "<center><small>Image from Stable-Baselines3 repository</small></center>\n",
    "\n",
    "---\n",
    "This notebook uses the [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3) library to train a [PPO](https://openai.com/blog/openai-baselines-ppo/) reinforcement learning agent on [GFootball Academy](https://github.com/google-research/football/tree/master/gfootball/scenarios) scenarios, applying the architecture from the paper \"[Google Research Football: A Novel Reinforcement Learning Environment](https://arxiv.org/abs/1907.11180)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# sys.path.append(\"../imitation_learning\")\n",
    "import os\n",
    "import base64\n",
    "import pickle\n",
    "import zlib\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from torch import nn, tensor\n",
    "from collections import deque\n",
    "from gym.spaces import Box, Discrete\n",
    "# from kaggle_environments import make\n",
    "# from kaggle_environments.envs.football.helpers import *\n",
    "from gfootball.env import create_environment, observation_preprocessing, wrappers\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv\n",
    "from stable_baselines3.common.policies import BasePolicy, register_policy\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from datetime import date\n",
    "# from visualizer import visualize\n",
    "from matplotlib import pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "import torch\n",
    "from models.MlpClassifierModel import MlpClassifierModel\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.manual_seed(torch.initial_seed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Football Gym\n",
    "> [Stable-Baselines3: Custom Environments](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)<br/>\n",
    "> [SEED RL Agent](https://www.kaggle.com/piotrstanczyk/gfootball-train-seed-rl-agent): stacked observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "class FootballGym(gym.Env):\n",
    "    spec = None\n",
    "    metadata = None\n",
    "#     metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, config=None, render=False, rewards='scoring'):\n",
    "        super(FootballGym, self).__init__()\n",
    "        env_name = \"academy_empty_goal_close\"\n",
    "#         rewards = \"scoring,checkpoints\"\n",
    "\n",
    "        rewards = rewards\n",
    "        if config is not None:\n",
    "            env_name = config.get(\"env_name\", env_name)\n",
    "            rewards = config.get(\"rewards\", rewards)\n",
    "        self.env = create_environment(\n",
    "            env_name=env_name,\n",
    "            stacked=False,\n",
    "            representation=\"simple115v2\",\n",
    "            rewards = rewards,\n",
    "            write_goal_dumps=False,\n",
    "            write_full_episode_dumps=False,\n",
    "            render=render,\n",
    "            write_video=False,\n",
    "            dump_frequency=1,\n",
    "            logdir=\".\",\n",
    "            extra_players=None,\n",
    "            number_of_left_players_agent_controls=1,\n",
    "            number_of_right_players_agent_controls=0)\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.reward_range = (-1, 1)\n",
    "        self.obs_stack = deque([], maxlen=4)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.obs_stack.clear()\n",
    "        obs = self.env.reset()\n",
    "#         obs = self.transform_obs(obs)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step([action])\n",
    "#         obs = self.transform_obs(obs)\n",
    "        return obs, float(reward), done, info\n",
    "    \n",
    "# check_env(env=FootballGym(), warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Football CNN\n",
    "> [Stable-Baselines3: Custom Policy Network](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html)<br/>\n",
    "> [Google Research Football: A Novel Reinforcement Learning Environment](https://arxiv.org/abs/1907.11180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> [Stable-Baselines3: PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)<br/>\n",
    "> [Stable-Baselines3: Vectorized Environments](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html)<br/>\n",
    "> [Stable-Baselines3: Custom Policy Network](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html)<br/>\n",
    "> [GFootball: A Novel Reinforcement Learning Environment](https://arxiv.org/abs/1907.11180)<br/>\n",
    "> [GFootball: Academy Scenarios](https://github.com/google-research/football/tree/master/gfootball/scenarios)<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpClassifierModel(pl.LightningModule):\n",
    "    def __init__(self, hparams, input_size: int = 115, p_dropout: float = 0.25, num_classes:int = 19):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hparams.update(hparams)\n",
    "        # self.hidden_size = hparams.get('hidden_size', 128)\n",
    "#         self.hidden_size = hparams['hidden_size']\n",
    "        self.lr = hparams['lr']\n",
    "        self.batch_size = hparams['batch_size']\n",
    "        self.activation = hparams['activation']\n",
    "        hidden_size = hparams['hidden_size']\n",
    "        if self.activation == 'LeakyReLU':\n",
    "            act_func = nn.LeakyReLU()\n",
    "        elif self.activation == 'ReLU':\n",
    "            act_func = nn.ReLU()\n",
    "#         else:\n",
    "#             act_func = nn.PReLU()\n",
    "        else:\n",
    "            act_func = nn.GELU()\n",
    "            \n",
    "        self.model = nn.Sequential(\n",
    "            # nn.BatchNorm1d(input_dim, affine=False),\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            act_func,\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            act_func,\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            act_func,\n",
    "            nn.Dropout(p_dropout),\n",
    "#             nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(module.weight)\n",
    "                torch.nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm1d):\n",
    "                if module.affine:\n",
    "                    torch.nn.init.constant_(module.weight, 1)\n",
    "                    module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        return self.model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootballMLP(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=115):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        self.mlp = MlpClassifierModel(hparams, input_size=115, p_dropout=0.25, num_classes=19)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        return self.mlp(input_tensor)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "hparams['hidden_size'] = 1024\n",
    "hparams['lr'] = 2e-3\n",
    "hparams['lr_decay_rate'] = 0.25\n",
    "hparams['batch_size'] = 256\n",
    "hparams['activation'] = 'GELU'\n",
    "# hparams['activation'] = 'ReLU'\n",
    "# model = MLPModel(hparams).to('cuda')\n",
    "# model = MlpClassifierModel(hparams).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class DDQN(DQN):\n",
    "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
    "        # Switch to train mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(True)\n",
    "        # Update learning rate according to schedule\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "\n",
    "        losses = []\n",
    "        for _ in range(gradient_steps):\n",
    "            ### YOUR CODE HERE\n",
    "            # Sample replay buffer\n",
    "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n",
    "\n",
    "            # Do not backpropagate gradient to the target network\n",
    "            with th.no_grad():\n",
    "#                 print(f\"replay data actions: {replay_data.actions.shape} and data: {replay_data.actions}\")\n",
    "                # Compute the next Q-values using the target network\n",
    "                next_q_values = self.q_net_target(replay_data.next_observations)\n",
    "#                 print(f\"Next Q values shape calculated by target net: {next_q_values.shape}\")\n",
    "                # Decouple action selection from value estimation\n",
    "                # Compute q-values for the next observation using the online q net\n",
    "                next_q_values_online = self.q_net(replay_data.next_observations)\n",
    "#                 print(f\"Next Q values shape calculated by Online net: {next_q_values_online.shape}\")\n",
    "                # Select action with online network\n",
    "                next_actions_online = next_q_values_online.argmax(dim=1)\n",
    "#                 print(f\"Next Actions shape calculated by Online net: {next_actions_online.shape}\")\n",
    "                # Estimate the q-values for the selected actions using target q network\n",
    "                next_q_values = th.gather(next_q_values, dim=1, index=next_actions_online.unsqueeze(-1))\n",
    "#                 print(f\"Next Q values calculated by Target net from the selected actions: {next_q_values.shape}\")\n",
    "               \n",
    "                # 1-step TD target\n",
    "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
    "\n",
    "            # Get current Q-values estimates\n",
    "            current_q_values = self.q_net(replay_data.observations)\n",
    "\n",
    "            # Retrieve the q-values for the actions from the replay buffer\n",
    "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
    "\n",
    "            # Check the shape\n",
    "            assert current_q_values.shape == target_q_values.shape\n",
    "\n",
    "            # Compute loss (L2 or Huber loss)\n",
    "            loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
    "\n",
    "            ### END OF YOUR CODE\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Optimize the q-network\n",
    "            self.policy.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip gradient norm\n",
    "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "            self.policy.optimizer.step()\n",
    "\n",
    "        # Increase update counter\n",
    "        self._n_updates += gradient_steps\n",
    "\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/loss\", np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "scenarios = {0: \"academy_empty_goal_close\",\n",
    "             1: \"academy_empty_goal\",\n",
    "             2: \"academy_run_to_score\",\n",
    "             3: \"academy_run_to_score_with_keeper\",\n",
    "             4: \"academy_pass_and_shoot_with_keeper\",\n",
    "             5: \"academy_run_pass_and_shoot_with_keeper\",\n",
    "             6: \"academy_3_vs_1_with_keeper\",\n",
    "             7: \"academy_corner\",\n",
    "             8: \"academy_counterattack_easy\",\n",
    "             9: \"academy_counterattack_hard\",\n",
    "             10: \"academy_single_goal_versus_lazy\",\n",
    "             11: \"11_vs_11_kaggle\",\n",
    "             12: \"11_vs_11_stochastic\",\n",
    "             13: \"11_vs_11_easy_stochastic\",\n",
    "             14: \"11_vs_11_hard_stochastic\"}\n",
    "\n",
    "scenario_name = scenarios[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment creation and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "def make_env(config: dict, rank: int, log_save_dir: str, seed: int = 42) -> Callable:\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "    \n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    :return: (Callable)\n",
    "    \"\"\"\n",
    "    def _init() -> gym.Env:\n",
    "        env = FootballGym(config)\n",
    "        log_file = os.path.join(log_save_dir, str(rank))\n",
    "        env = Monitor(env, log_file, allow_early_resets=True)\n",
    "        env.seed(seed + rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the vectorized training environmewnt and also creating the direcotry for logging\n",
    "\n",
    "timestamp = time.strftime('%d-%m-%Y-%H-%M-%S', time.localtime())\n",
    "print(timestamp)\n",
    "\n",
    "n_envs = 8\n",
    "config={\"env_name\":scenario_name, 'reward':'baal, checkpoint'}\n",
    "log_save_dir = os.path.join(\"../logs/dqn_logs\", timestamp)\n",
    "print(f\"Log dir: {log_save_dir}\")\n",
    "os.mkdir(log_save_dir)\n",
    "train_env = SubprocVecEnv([make_env(config, rank=i, log_save_dir=log_save_dir) for i in range(n_envs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Vanilla DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # This initialization concentrates most of the layers into the feature extractor\n",
    "# # and then leaves only a single layer for the prediction part\n",
    "\n",
    "# policy_kwargs = dict(features_extractor_class=FootballMLP,\n",
    "#                      features_extractor_kwargs=dict(features_dim=1024),\n",
    "#                     net_arch = [],\n",
    "#                     )\n",
    "# model_name = \"dqn\"\n",
    "# model = DQN(policy=\"MlpPolicy\", \n",
    "#             env=train_env, \n",
    "#             policy_kwargs=policy_kwargs, \n",
    "#             verbose=1,\n",
    "#             exploration_initial_eps=0.00,\n",
    "#             exploration_final_eps=0.0,\n",
    "#             target_update_interval=15000,\n",
    "# #             learning_rate=hparams['lr'],\n",
    "# #             batch_size=hparams['batch_size'],\n",
    "#             seed=42,\n",
    "#             tensorboard_log='tb_logs_DQN',\n",
    "#             learning_starts=100000,\n",
    "#            )\n",
    "# model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Configuring the DQN net like SB3's network structure\n",
    "# # Only one layer for the feature extraction and the rest for the Q value computation\n",
    "# from stable_baselines3 import DQN\n",
    "\n",
    "# policy_kwargs = dict(\n",
    "#     net_arch = [1024, 1024, 1024],\n",
    "#     activation_fn = torch.nn.GELU\n",
    "# )\n",
    "# model_name = \"dqn\"\n",
    "# model = DQN(policy=\"MlpPolicy\", \n",
    "#             env=train_env, \n",
    "#             policy_kwargs=policy_kwargs, \n",
    "#             verbose=1,\n",
    "#             exploration_initial_eps=0.00,\n",
    "#             exploration_final_eps=0.0,\n",
    "#             target_update_interval=15000,\n",
    "# #             learning_rate=hparams['lr'],\n",
    "# #             batch_size=hparams['batch_size'],\n",
    "#             seed=42,\n",
    "#             tensorboard_log='tb_logs_DQN',\n",
    "#             learning_starts=100000,\n",
    "#            )\n",
    "# model.policy\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DDQN\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch = [1024, 1024, 1024],\n",
    "    activation_fn = torch.nn.ReLU\n",
    ")\n",
    "\n",
    "# policy_kwargs = dict(features_extractor_class=FootballMLP,\n",
    "#                      features_extractor_kwargs=dict(features_dim=1024),\n",
    "#                     net_arch = [],\n",
    "#                     )\n",
    "model_name = \"ddqn\"\n",
    "model = DDQN(policy=\"MlpPolicy\", \n",
    "            env=train_env, \n",
    "            policy_kwargs=policy_kwargs, \n",
    "            verbose=1,\n",
    "            exploration_initial_eps=0.00,\n",
    "            exploration_final_eps=0.00,\n",
    "            target_update_interval=150000,\n",
    "#             learning_rate=0.0000001,\n",
    "#             batch_size=hparams['batch_size'],\n",
    "            seed=42,\n",
    "            tensorboard_log='tb_logs_DDQN',\n",
    "            train_freq=3002,\n",
    "#             learning_starts=100000,\n",
    "           )\n",
    "\n",
    "# With cartpole initialization\n",
    "# model = DDQN(policy=\"MlpPolicy\", \n",
    "#             env=train_env, \n",
    "#             learning_rate=2.3e-3,\n",
    "#             batch_size=64,\n",
    "#             buffer_size=100000,\n",
    "#             learning_starts=1000,\n",
    "#             gamma=0.99,\n",
    "#             target_update_interval=10,\n",
    "#             train_freq=256,\n",
    "#             gradient_steps=128,\n",
    "#             exploration_fraction=0.16,\n",
    "#             exploration_final_eps=0.04,\n",
    "#             policy_kwargs=policy_kwargs,\n",
    "#              tensorboard_log='tb_logs_DDQN',\n",
    "#              seed=42,\n",
    "#              verbose=1,\n",
    "#            )\n",
    "model.policy\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"/media/ssk/DATA/GRP_code/gr_football_analytics/notebooks/lightning_logs/version_27/checkpoints/epoch=208-step=681548.ckpt\"\n",
    "checkpoint_path = \"/media/ssk/DATA/GRP_code/gr_football_analytics/notebooks/lightning_logs/version_38/checkpoints/epoch=146-step=479366.ckpt\"\n",
    "checkpoint_dict = torch.load(checkpoint_path)\n",
    "checkpoint_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_keys_todqn_keys_dict = {}\n",
    "sd_dqn_model = model.policy.state_dict()\n",
    "count = 0\n",
    "for mlp_key, dqn_key in zip(checkpoint_dict['state_dict'].keys(), model.policy.q_net.state_dict().keys()):\n",
    "    sd_dqn_model['q_net.' + dqn_key] = checkpoint_dict['state_dict'][mlp_key]\n",
    "for mlp_key, dqn_key in zip(checkpoint_dict['state_dict'].keys(), model.policy.q_net_target.state_dict().keys()):\n",
    "    sd_dqn_model['q_net_target.' + dqn_key] = checkpoint_dict['state_dict'][mlp_key]\n",
    "    \n",
    "model.policy.load_state_dict(sd_dqn_model)\n",
    "\n",
    "# Check the model's weights after loading the weights from IL agent\n",
    "model.policy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, param in enumerate(model.policy.parameters()):\n",
    "# #     if param.shape == torch.Size([19, 1024]) or param.shape == torch.Size([19]):\n",
    "#     if param.shape == torch.Size([19]):\n",
    "#         print(param.shape, param.requires_grad)\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False\n",
    "#         #     print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, param in enumerate(model.policy.parameters()):\n",
    "# #     param.requires_grad = True\n",
    "#     print(param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training\n",
    "> [Stable-Baselines3: Examples](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html)<br/>\n",
    "> [Stable-Baselines3: Callbacks](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "class ProgressBar(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(ProgressBar, self).__init__(verbose)\n",
    "        self.pbar = None\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        factor = np.ceil(self.locals['total_timesteps'] / n_steps)\n",
    "        print(f\"self.locals['total_timesteps']:{self.locals['total_timesteps']}, n_steps: {n_steps}\")\n",
    "        n = 1\n",
    "        try:\n",
    "            n = len(self.training_env.envs)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                n = len(self.training_env.remotes)\n",
    "            except AttributeError:\n",
    "                n = 1\n",
    "        total = int(n_steps * factor / n)\n",
    "        self.pbar = tqdm(total=total)\n",
    "\n",
    "    def _on_rollout_start(self):\n",
    "        self.pbar.refresh()\n",
    "\n",
    "    def _on_step(self):\n",
    "        self.pbar.update(1)\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        self.pbar.refresh()\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        self.pbar.close()\n",
    "        self.pbar = None\n",
    "\n",
    "progressbar = ProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback = EvalCallback(train_env, log_path = log_save_dir, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_epochs = 30\n",
    "n_steps = 3002\n",
    "total_timesteps = n_steps * n_envs * total_epochs\n",
    "model.learn(total_timesteps=total_timesteps, callback=progressbar, log_interval=8, tb_log_name='ddqn_scoring_il_init_30_epochs_train_freq_3002_added_KL_loss_epsilon_0')\n",
    "\n",
    "\n",
    "saved_model_name = model_name + '_gfootball_' + str(n_envs) + \"_\" + timestamp\n",
    "model.save(f\"../models/{model_name}/{saved_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir tb_logs_DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, param in enumerate(model.policy.parameters()):\n",
    "#     if param.shape == torch.Size([19, 1024]) or param.shape == torch.Size([19]):\n",
    "#         print(param.shape, param.requires_grad)\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False\n",
    "#         #     print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, param in enumerate(model.policy.parameters()):\n",
    "# #     param.requires_grad = True\n",
    "#     print(param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.randn([2, 3])\n",
    "# a.shape == torch.Size([2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the rewards per timestep and per episode\n",
    "\n",
    "plt.style.use(['seaborn-whitegrid'])\n",
    "results_plotter.plot_results([log_save_dir], total_timesteps, results_plotter.X_TIMESTEPS, \"GFootball Timesteps\")\n",
    "# plt.savefig('../figures/dqn/rewards_per_timestamp_dqn_with_my_policy.png')\n",
    "results_plotter.plot_results([log_save_dir], total_timesteps, results_plotter.X_EPISODES, \"GFootball Episodes\")\n",
    "# plt.savefig('../figures/dqn/rewards_per_episode_dqn_with_my_policy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the episodic reward with line\n",
    "\n",
    "x, y = results_plotter.ts2xy(results_plotter.load_results(log_save_dir), 'timesteps')  # Organising the logged results in to a clean format for plotting.\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "plt.plot(x,y)\n",
    "plt.ylim([-10, 10])\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Episode Rewards')\n",
    "plt.savefig('../figures/dqn/episode_rewards_50_epochs_with_IL_diff_net.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the rolling mean reward per environment\n",
    "\n",
    "plt.style.use(['seaborn-whitegrid'])\n",
    "log_files = [os.path.join(log_save_dir, f\"{i}.monitor.csv\") for i in range(n_envs)]\n",
    "\n",
    "nrows = np.ceil(n_envs/2)\n",
    "fig = plt.figure(figsize=(8, 2 * nrows))\n",
    "for i, log_file in enumerate(log_files):\n",
    "    if os.path.isfile(log_file):\n",
    "        df = pd.read_csv(log_file, skiprows=1)\n",
    "        plt.subplot(nrows, 2, i+1, label=log_file)\n",
    "        df['r'].rolling(window=5).mean().plot(title=f\"Rewards: Env {i}\")\n",
    "        plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean episodic reward\n",
    "\n",
    "df = pd.read_csv('data_for_figures/run-ddqn_3-tag-rollout_ep_rew_mean.csv')\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "df.plot(x ='Step', y='Value')\n",
    "# plt.savefig('../figures/dqn/mean_ep_reward_with_IL_50_epochs_diff_net.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x, y = results_plotter.ts2xy(results_plotter.load_results('../logs/dqn_logs/10-05-2022-11-15-40'), 'timesteps')  # Organising the logged results in to a clean format for plotting.\n",
    "# fig = plt.figure(figsize=(20, 16))\n",
    "# plt.plot(x,y)\n",
    "# plt.ylim([-10, 10])\n",
    "# plt.xlabel('Timesteps')\n",
    "# plt.ylabel('Episode Rewards')\n",
    "# # plt.savefig('../figures/dqn/episode_rewards_50_epochs_without_IL.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use(['seaborn-whitegrid'])\n",
    "# log_files = ['../logs/dqn_logs/10-05-2022-11-15-40', '../logs/dqn_logs/10-05-2022-12-15-33']\n",
    "\n",
    "# nrows = 1\n",
    "# fig = plt.figure(figsize=(8, 2))\n",
    "# for i, log_file in enumerate(log_files):\n",
    "# #     if os.path.isfile(log_file):\n",
    "#     x, y = results_plotter.ts2xy(results_plotter.load_results(log_file), 'timesteps')  # Organising the logged results in to a clean format for plotting.\n",
    "    \n",
    "# #         df = pd.read_csv(log_file, skiprows=1)\n",
    "#     plt.subplot(nrows, 2, i+1, label='log_file', title=\"hello\")\n",
    "#     plt.plot(x,y)\n",
    "# #         df['r'].rolling(window=5).mean().plot(title=f\"Rewards: Env {i}\")\n",
    "#     plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use(['seaborn-whitegrid'])\n",
    "# log_files = ['../logs/dqn_logs/10-05-2022-11-15-40', '../logs/dqn_logs/10-05-2022-12-15-33']\n",
    "\n",
    "# nrows = 1\n",
    "# fig = plt.figure(figsize=(16, 4))\n",
    "# #     if os.path.isfile(log_file):\n",
    "# x_with_il, y_with_il = results_plotter.ts2xy(results_plotter.load_results(log_files[0]), 'timesteps')  # Organising the logged results in to a clean format for plotting.\n",
    "# x_wo_il, y_wo_il = results_plotter.ts2xy(results_plotter.load_results(log_files[1]), 'timesteps')  # Organising the logged results in to a clean format for plotting.\n",
    "    \n",
    "# #         df = pd.read_csv(log_file, skiprows=1)\n",
    "# plt.subplot(nrows, 2, 1, label='log_file', title=f\"With IL initialization after 50 epochs\")\n",
    "# plt.plot(x_with_il, y_with_il)\n",
    "# plt.subplot(nrows, 2, 2, label='log_file', title=f\"Without IL initialization after 50 epochs\")\n",
    "# plt.plot(x_wo_il, y_wo_il)\n",
    "# #         df['r'].rolling(window=5).mean().plot(title=f\"Rewards: Env {i}\")\n",
    "# plt.tight_layout()\n",
    "# # plt.show()\n",
    "# plt.savefig('../figures/dqn/compariosn_of_initialization.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use(['seaborn-whitegrid'])\n",
    "# log_files = ['data_for_figures/run-DQN_22-tag-rollout_ep_rew_mean.csv', 'data_for_figures/run-DQN_23-tag-rollout_ep_rew_mean.csv']\n",
    "# names = ['with IL', 'without IL']\n",
    "# nrows = 1\n",
    "# fig = plt.figure(figsize=(16, 4 * nrows))\n",
    "\n",
    "# for i, log_file in enumerate(log_files):\n",
    "#     df = pd.read_csv(log_file)\n",
    "#     plt.subplot(nrows, 2, i+1, label='log_file', title=names[i])\n",
    "#     plt.plot(df['Step'], df['Value'])\n",
    "# #     df.plot(x='Step', y='Value', title=f\"Hello\")\n",
    "# #     plt.tight_layout()\n",
    "# # plt.show()\n",
    "# plt.savefig('../figures/dqn/comparison_mean_ep_rew_with_and_wo_IL_init.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Agent\n",
    "> [Stable-Baselines: Exporting Models](https://stable-baselines.readthedocs.io/en/master/guide/export.html)<br/>\n",
    "> [Stable-Baselines: Converting a Model into PyTorch](https://github.com/hill-a/stable-baselines/issues/372)<br/>\n",
    "> [Connect4: Make Submission with Stable-Baselines3](https://www.kaggle.com/toshikazuwatanabe/connect4-make-submission-with-stable-baselines3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# from stable_baselines3 import DQN\n",
    "# model = DQN.load(\"dqn_gfootball\")\n",
    "# test_env = FootballGym({\"env_name\":scenario_name}, render=False)\n",
    "# obs = test_env.reset()\n",
    "# done = False\n",
    "# rewards = []\n",
    "# goals_scored, goals_conceded = 0, 0\n",
    "# while not done:\n",
    "# #     action, state = model.predict(obs, deterministic=True)\n",
    "#     action, state = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, info = test_env.step(action)\n",
    "#     if reward < 0:\n",
    "#         goals_conceded += reward\n",
    "#     elif reward > 0:\n",
    "#         goals_scored += reward\n",
    "#     rewards.append(reward)\n",
    "# #     print(f\"{Action(action).name.ljust(16,' ')}\\t{round(reward,2)}\\t{info}\\t{done}\")\n",
    "#     print(f\"{action_set[action].ljust(16,' ')}\\t{round(reward,2)}\\t{info}\\t{done}\")\n",
    "# print(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def play_match(test_env):\n",
    "#     obs = test_env.reset()\n",
    "#     env_steps = 0\n",
    "#     match_reward = 0\n",
    "#     done = False\n",
    "#     my_agent = 0\n",
    "#     ai_agent = 0\n",
    "# #     model.eval()\n",
    "#     while not done:\n",
    "#         obs = torch.tensor(obs).to('cuda')\n",
    "#         obs = obs.reshape((1, obs.shape[0])).to('cuda')\n",
    "#         outputs = model(obs)\n",
    "#         _, action = torch.max(outputs.data, 1)\n",
    "#         obs, reward, done, info = test_env.step(action.item())\n",
    "# #         print(type(obs))\n",
    "# #         match_reward += reward\n",
    "#         if reward > 0:\n",
    "#             my_agent += 1\n",
    "#         elif reward < 0:\n",
    "#             ai_agent += 1\n",
    "# #             print(f\"Step: {str(env_steps).ljust(10, ' ')}\\t{str(action_set[action.item()]).ljust(10, ' ')}\\t{round(reward,2)}\\t{info}\")\n",
    "#         env_steps += 1\n",
    "#         if (env_steps+1) % 3001  == 0:\n",
    "# #             print(f\"Match reward: {match_reward}\")\n",
    "#             return my_agent, ai_agent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import DQN\n",
    "# model = DQN.load(\"dqn_gfootball\")\n",
    "# test_env = FootballGym({\"env_name\":scenario_name}, render=False)\n",
    "# obs = test_env.reset()\n",
    "# done = False\n",
    "# rewards = []\n",
    "# while not done:\n",
    "#     action, state = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, info = test_env.step(action)\n",
    "#     rewards.append(reward)\n",
    "#     print(f\"{Action(action).name.ljust(16,' ')}\\t{round(reward,2)}\\t{info}\\t{done}\")\n",
    "    \n",
    "#     if done:\n",
    "#         ep_rew = sum(rewards)\n",
    "#         ep_len = len(rewards)\n",
    "#         print(ep_rew, ep_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "%%writefile submission.py\n",
    "import base64\n",
    "import pickle\n",
    "import zlib\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch import nn, tensor\n",
    "from collections import deque\n",
    "from gfootball.env import observation_preprocessing\n",
    "\n",
    "state_dict = _STATE_DICT_\n",
    "\n",
    "state_dict = pickle.loads(zlib.decompress(base64.b64decode(state_dict)))\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out += residual\n",
    "        return out\n",
    "    \n",
    "class PyTorchCnnPolicy(nn.Module):\n",
    "    global state_dict\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            conv3x3(in_channels=16, out_channels=32),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, ceil_mode=False),\n",
    "            ResidualBlock(in_channels=32, out_channels=32),\n",
    "            ResidualBlock(in_channels=32, out_channels=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "          nn.Linear(in_features=52640, out_features=256, bias=True),\n",
    "          nn.ReLU(),\n",
    "        )\n",
    "        self.action_net = nn.Sequential(\n",
    "          nn.Linear(in_features=256, out_features=19, bias=True),\n",
    "          nn.ReLU(),\n",
    "        )\n",
    "        self.out_activ = nn.Softmax(dim=1)\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = tensor(x).float() / 255.0  # normalize\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()  # 1 x channels x height x width\n",
    "        x = self.cnn(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.action_net(x)\n",
    "        x = self.out_activ(x)\n",
    "        return int(x.argmax())\n",
    "    \n",
    "obs_stack = deque([], maxlen=4)\n",
    "def transform_obs(raw_obs):\n",
    "    global obs_stack\n",
    "    obs = raw_obs['players_raw'][0]\n",
    "    obs = observation_preprocessing.generate_smm([obs])\n",
    "    if not obs_stack:\n",
    "        obs_stack.extend([obs] * 4)\n",
    "    else:\n",
    "        obs_stack.append(obs)\n",
    "    obs = np.concatenate(list(obs_stack), axis=-1)\n",
    "    return obs\n",
    "\n",
    "policy = PyTorchCnnPolicy()\n",
    "policy = policy.float().to('cpu').eval()\n",
    "def agent(raw_obs):\n",
    "    obs = transform_obs(raw_obs)\n",
    "    action = policy(obs)\n",
    "    return [action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_gfootball\")\n",
    "_state_dict = model.policy.to('cpu').state_dict()\n",
    "state_dict = {\n",
    "    \"cnn.0.weight\":_state_dict['features_extractor.cnn.0.weight'], \n",
    "    \"cnn.0.bias\":_state_dict['features_extractor.cnn.0.bias'], \n",
    "    \"cnn.2.conv1.weight\":_state_dict['features_extractor.cnn.2.conv1.weight'], \n",
    "    \"cnn.2.conv1.bias\":_state_dict['features_extractor.cnn.2.conv1.bias'],\n",
    "    \"cnn.2.conv2.weight\":_state_dict['features_extractor.cnn.2.conv2.weight'], \n",
    "    \"cnn.2.conv2.bias\":_state_dict['features_extractor.cnn.2.conv2.bias'], \n",
    "    \"cnn.3.conv1.weight\":_state_dict['features_extractor.cnn.3.conv1.weight'], \n",
    "    \"cnn.3.conv1.bias\":_state_dict['features_extractor.cnn.3.conv1.bias'], \n",
    "    \"cnn.3.conv2.weight\":_state_dict['features_extractor.cnn.3.conv2.weight'], \n",
    "    \"cnn.3.conv2.bias\":_state_dict['features_extractor.cnn.3.conv2.bias'], \n",
    "    \"linear.0.weight\":_state_dict['features_extractor.linear.0.weight'], \n",
    "    \"linear.0.bias\":_state_dict['features_extractor.linear.0.bias'], \n",
    "    \"action_net.0.weight\":_state_dict['action_net.weight'],\n",
    "    \"action_net.0.bias\":_state_dict['action_net.bias'],\n",
    "}\n",
    "state_dict = base64.b64encode(zlib.compress(pickle.dumps(state_dict)))\n",
    "with open('submission.py', 'r') as file:\n",
    "    src = file.read()\n",
    "src = src.replace(\"_STATE_DICT_\", f\"{state_dict}\")\n",
    "with open('submission.py', 'w') as file:\n",
    "    file.write(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "kaggle_env = make(\"football\", debug = False,\n",
    "                  configuration={\"scenario_name\": scenario_name, \n",
    "                                 \"running_in_notebook\": True,\n",
    "                                 \"save_video\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "output = kaggle_env.run([\"submission.py\", \"do_nothing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "scores = output[-1][0][\"observation\"][\"players_raw\"][0][\"score\"]\n",
    "print(\"Scores  {0} : {1}\".format(*scores))\n",
    "print(\"Rewards {0} : {1}\".format(output[-1][0][\"reward\"], output[-1][1][\"reward\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "viz = visualize(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Modified [Human Readable Visualization](https://www.kaggle.com/jaronmichal/human-readable-visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "HTML(viz.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Checkpoints\n",
    "\n",
    "0. [academy_empty_goal_close @ 800K steps](https://www.kaggle.com/kwabenantim/gfootball-stable-baselines3?scriptVersionId=45569809#Test-Agent) (Nature CNN)<br/>\n",
    "1. [academy_empty_goal @ 800K steps](https://www.kaggle.com/kwabenantim/gfootball-stable-baselines3?scriptVersionId=45639135#Test-Agent) (Nature CNN)<br/>\n",
    "2. [academy_run_to_score @ 800K steps](https://www.kaggle.com/kwabenantim/gfootball-stable-baselines3?scriptVersionId=45941674#Test-Agent) (Nature CNN)<br/>\n",
    "3. [academy_run_to_score_with_keeper @ 800K steps](https://www.kaggle.com/kwabenantim/gfootball-stable-baselines3?scriptVersionId=45703399#Test-Agent) (Nature CNN)<br/>\n",
    "4. [academy_pass_and_shoot_with_keeper @ 800K steps](https://www.kaggle.com/kwabenantim/gfootball-stable-baselines3?scriptVersionId=45716494#Test-Agent) (Nature CNN)<br/>\n",
    "5. [academy_run_pass_and_shoot_with_keeper @ 1.6M steps](https://www.kaggle.com/kwabenantim/gfootball-stable-baselines3?scriptVersionId=46590578#Testing) (Nature CNN)<br/>\n",
    "6. [academy_3_vs_1_with_keeper @ 500K steps](https://www.kaggle.com/kwabenantim/gfootball-stable-baselines3?scriptVersionId=46843278#Testing) (GFootball CNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
